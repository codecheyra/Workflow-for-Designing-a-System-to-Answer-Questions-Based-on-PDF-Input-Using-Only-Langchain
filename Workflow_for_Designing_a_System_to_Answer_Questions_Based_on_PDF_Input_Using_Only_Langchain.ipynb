{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Input is a PDF file and questions, Output is an answers based on PDF file.***"
      ],
      "metadata": {
        "id": "GD04-5lJ_4aH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Ingest the PDF File**\n",
        "Use Langchain to read and extract text from the PDF."
      ],
      "metadata": {
        "id": "KQDbzxtYACpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/your_pdf.pdf\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "AeZOme3JAE1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Preprocess the Text**\n",
        "Clean and preprocess the extracted text (e.g., remove unnecessary characters, format sections). Segment the text into meaningful chunks."
      ],
      "metadata": {
        "id": "JSZvVEfsAKCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "char_text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "doc_texts = char_text_splitter.split_documents(docs)\n",
        "for chunk in doc_texts:\n",
        "  chunk.page_content=\"{{{{CHUNK_STARTING}}}}\"+chunk.page_content+\"{{{{CHUNK_ENDING}}}}\""
      ],
      "metadata": {
        "id": "NfCCkUSbANiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Create Vector Store**\n",
        "Generate embeddings for the text segments and store them in a vector store for efficient similarity search and retrieval."
      ],
      "metadata": {
        "id": "X5XGoOMUAQnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"roberta-base-nli-stsb-mean-tokens\")\n",
        "vStore = FAISS.from_documents(doc_texts, embeddings)"
      ],
      "metadata": {
        "id": "3CZJ6am1ATYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Initialize and Configure the LLM**\n",
        "Initialize and configure the LLM within Langchain using the pre-trained model and the vector store. Apply appropriate hyperparameters and settings."
      ],
      "metadata": {
        "id": "oMpWxPyeAV5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "from langchain.llms import CTransformers\n",
        "from langchain.chains import VectorDBQA\n",
        "\n",
        "def load_llm():\n",
        "    accelerator = Accelerator()\n",
        "    config = {'max_new_tokens': 1048, 'repetition_penalty': 1.1, 'context_length': 8000, 'temperature':0.3, 'gpu_layers':50}\n",
        "    llm = CTransformers(model=\"/content/llama-2-7b.Q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50, config=config)\n",
        "    llm, config = accelerator.prepare(llm, config)\n",
        "    return llm\n",
        "model = VectorDBQA.from_chain_type(llm=load_llm(), chain_type=\"stuff\", vectorstore=vStore, k=2)"
      ],
      "metadata": {
        "id": "IgSuEqojAZCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Generate Q&A Pairs**\n",
        "Utilize the initialized LLM to generate potential Q&A pairs based on the text segments. Format and save the generated Q&A pairs."
      ],
      "metadata": {
        "id": "cHa7CAceAcNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "def format(s):\n",
        "    return s.split(\"{{{{CHUNK_STARTING}}}}\")\n",
        "\n",
        "base_folder = 'static/output/'\n",
        "if not os.path.isdir(base_folder):\n",
        "    os.mkdir(base_folder)\n",
        "output_file = base_folder + \"QA.csv\"\n",
        "\n",
        "def answer_generator():\n",
        "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "        csv_writer.writerow([\"Question\", \"Answer\"])\n",
        "        i = 0\n",
        "        for q in questions:\n",
        "            i += 1\n",
        "            a = format(model.run(q))[0]\n",
        "            csv_writer.writerow([q, a])\n",
        "            print(\"Question:\", q)\n",
        "            print(\"Answer:\", a)\n",
        "            if i == 15:\n",
        "                break\n",
        "answer_generator()"
      ],
      "metadata": {
        "id": "cfxblP7yAfzy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}